{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Algorithm Questions**"
      ],
      "metadata": {
        "id": "-jVIGCiFPP_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. How does regularization (L1 and L2) help in preventing overfitting?**\n",
        "\n",
        "Overfitting - where the  model gets trained too much with the given features that it learns from noise also, hence training accuracy will be high but testing accuracy is less\n",
        "\n",
        "It can be prevented using ensemble learning or regularisation\n",
        "\n",
        "L1 and L2 regularisation --> penalises the weights of less important features or makes it to zero thus the model will not learn from those features, hence avoiding overfitting\n"
      ],
      "metadata": {
        "id": "Blj6Wq_RPaEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.Why is feature scaling important in gradient descent?**\n",
        "\n",
        "Feature scaling refers to converting all features on a common scale. If not done,  one or the other feature with high values contributes ot create a biased model thus affecting the accuracy.\n",
        "\n",
        "In gradient descent, as feature scaling is done, it helps in smooth transition to minima"
      ],
      "metadata": {
        "id": "Ok--OPZvQT5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Problem Solving**"
      ],
      "metadata": {
        "id": "OUnEFTZeQ9Yq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Given a dataset with missing values, how would you handle them before training an ML model?**\n",
        "\n",
        "Handling Missing Values\n",
        "\n",
        "**Step 1:** Once the dataset is loaded as pandas dataframe, check **dataset.info()** --> gives the basic information about the dataset - datatype, non-null values, memory, column names etc.\n",
        "\n",
        "**Step 2:** Check for missing values in the dataset using **dataset.isna().sum()** --> gives the number of missing values in each column.\n",
        "\n",
        "**If the percentage of missing values are more than 60% in a particular column, drop that column using dataset.dropna('column name', axis=1, inplace=True)**\n",
        "\n",
        "Else can be treated.\n",
        "\n",
        "**Step 3:** If there are outliers, treat the outliers by clipping to upper max and lower max and then go for missing values\n",
        "\n",
        "**Step 4:** If the **missing values are present in numerical column** Plot histogram and check for skewness\n",
        "\n",
        "  4.1 If the plot is **skewed, can fill the missing values with median**\n",
        "\n",
        "  4.2 If the values are **normally distributed, fill the missing values with mean**\n",
        "\n",
        "**Step 5:** If there are missing values in **categorical column**, can fill it using\n",
        "**mode**\n",
        "\n",
        "If there are empty spaces instead of null values in the column , can fill the empty spaces with np.nan and then start from step 2 above."
      ],
      "metadata": {
        "id": "NLspodFQRFuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.Design a pipeline for building a classification model. Include steps for data preprocessing.**\n",
        "\n",
        "**Step 1: Importing the data**\n",
        "\n",
        "  1.1 Import the neccessary libraries - pandas, matplotlib, sklearn, seaborn, numpy etc\n",
        "  \n",
        "  1.2 Load the dataset using pandas as df\n",
        "  \n",
        "  1.3 Check the basic information about the dataset using df.info()\n",
        "\n",
        "  1.4 Check the various columns present and understand the data\n",
        "\n",
        "**Step 2: Data Cleaning and Visualisation**\n",
        "  \n",
        "  2.1 Check for **outliers**, can visualise it using box plot and treat it by capping to max and min value\n",
        "  \n",
        "  2.2 Handling **Missing Values** by filling the missing values with mean, median or mode value.\n",
        "\n",
        "  2.3 Check for duplicated values in the dataset and remove it.\n",
        "\n",
        "  2.4 Can visualise the data using data visualisation tools like seaborn, matplolib\n",
        "\n",
        "**Step 3: Data Preprocessing and Building a Classification Model**\n",
        "\n",
        "  3.1 Decide the classifier that can be used based on whether it is a binary or multiclass classification problem.\n",
        "\n",
        "  If it is binary classification --> can use Logistic Regression else Naive Bayes, SVM, KNN, Decision Tree\n",
        "\n",
        "  **If the classification model chosen is a distance based algorithm like KNN classifier, Data scaling is needed**\n",
        "\n",
        "  3.2 If there are some **categorical columns to be included for building the model --> encoding** is done before building the model  -- includes one hot encoding, label and ordinal encoding depending on the need can be chosen\n",
        "\n",
        "  3.2 Define the X and y variables\n",
        "\n",
        "  3.3 **Data Preprocessing : Scale the X data using StandardScaler()** --> bring the values in various columns to a common scale, helps in improving the accuracy and reduces the biasness in the model.\n",
        "\n",
        "  3.4 If the **dataset is imbalanced, can balance it using SMOTE** else it affects the model's prediction\n",
        "\n",
        "  3.5 Split the data into train and test data using train_test_split\n",
        "\n",
        "  3.6 Build the model, fit the model and predict for the test data\n",
        "\n",
        "**Step 4: Evaluation of model**\n",
        "\n",
        "  4.1 Model can be evaluated using various metrics like accuracy, precision etc\n",
        "\n",
        "  Since it is a classification model, evaluate the model using **confusion matrix** --> matrix between the actual and predicted values. It shows how many are correct predictions and false predictions.\n",
        "\n",
        "  Other metrices include recall, f1 score etc.\n"
      ],
      "metadata": {
        "id": "ekfHcNweUzEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Coding**"
      ],
      "metadata": {
        "id": "kQeUFkjraEUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Write a Python script to implement a decision tree classifier using Scikit-learn.**\n"
      ],
      "metadata": {
        "id": "8J80VxV_aI-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model=DecisionTreeClassifier(criterion='entropy')\n",
        "model.fit(X_train,y_train) #X_train, y_train --> training data\n",
        "y_pred=model.predict(X_test) #X_test --> test data y_pred --> predicted y"
      ],
      "metadata": {
        "id": "4B5JQKKtaNDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.Given a dataset, write code to split the data into training and testing sets using an 80-20 split.**\n"
      ],
      "metadata": {
        "id": "Ydya0GayaxFu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vKdnHXCPLDC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test,y_train,y_test=train_test_split(X,y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Case Study**"
      ],
      "metadata": {
        "id": "b2MH9oThbtf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A company wants to predict employee attrition. What kind of ML problem is this? Which algorithms would you choose and why?**"
      ],
      "metadata": {
        "id": "YXhUxKhAb2eZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Employee Attrition is a binary classification problem - So logistic regression, KNN, SVM, Naive Bayes, Decision Tree can be used.\n",
        "\n",
        "The data neednot be linearly separable as the reasons may vary from employee to employee for staying in the company. As many factors like salary, promotion, worklife balance, work satisfaction etc contribute to it, their relationship with employee attrition cannot be linear always.\n",
        "\n",
        "Decision tree can be chosen as it works well with non-linear as well as non-scaled data too for this classification problem."
      ],
      "metadata": {
        "id": "WNtsniEpb82B"
      }
    }
  ]
}